{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW02 Report on HMM with Greedy and Viterbi Decoding\n",
    "-- By Ruijie Rao"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment gives you hands-on experience on using HMMs on part-ofspeech tagging. We will use the Wall Street Journal section of the Penn\n",
    "Treebank to build an HMM model for part-of-speech tagging. In the folder\n",
    "named data, there are three files: train, dev and test. In the files of train and\n",
    "dev, we provide you with the sentences with human-annotated part-of-speech\n",
    "tags. In the file of test, we provide only the raw sentences that you need to\n",
    "predict the part-of-speech tags. The data format is that, each line contains\n",
    "three items separated by the tab symbol ‘\\t’. The first item is the index of\n",
    "the word in the sentence. The second item is the word type and the third\n",
    "item is the corresponding part-of-speech tag. There will be a blank line at\n",
    "the end of one sentence."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Vocabulary\n",
    "\n",
    "In the first pass, I output the vocab using only \\<unk\\> tag for unknown words. After tweaking the threshold, I found 5 to generate the best accuracy scores. In the second pass, I significantly increased the accuracy by adding more pseudo-words including:\n",
    "- \\<-unk\\> for unknown with hyphens\n",
    "- \\<allcap-unk\\> for unknown with all capital letters\n",
    "- \\<Title-unk\\> are for those with capital first letter\n",
    "\n",
    "What is weird is that after using mor pseudo-words, the best threshold also decreases: allowing more words in the vocab while further raising the accuracy. I cannot figure out why, so if possible I would like someone to suggest an answer for me in the feedback.\n",
    "\n",
    "--Before my optimization using pseudo-words--\n",
    "\n",
    "What is the selected threshold for unknown words replacement?\n",
    "- 5\n",
    "\n",
    "What is the total size of your vocabulary?\n",
    "- 11688\n",
    "\n",
    "what is the total occurrences of the special token ‘< unk >’\n",
    "after replacement?\n",
    "- 50296\n",
    "\n",
    "--After my optimization using pseudo-words--\n",
    "\n",
    "What is the selected threshold for unknown words replacement?\n",
    "- 2\n",
    "\n",
    "What is the total size of your vocabulary?\n",
    "- 23183\n",
    "\n",
    "what is the total occurrences of the special token ‘< unk >’\n",
    "after replacement?\n",
    "- 20011 (including all unknown pseudo-words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Model Learning\n",
    "\n",
    "I approached greedy and viterbi in two different kind of ways: I transformed the dictionaries demanded in the task into matrices in Greedy decoding to make the operations more efficient using numpy mult; I went back to using the dictionaries to avoid the sparse spaces in the matrices during the multi-loop operations in the Viterbi Algorithm, which I assume would be a better choice than using matrices.\n",
    "\n",
    "How many transition and emission parameters in your HMM?\n",
    "- 1392 for transition dictionary\n",
    "- 30362 for emission dictionary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Greedy Decoding with HMM\n",
    "The optimization using pseudo-words was due to my exploration on the error set, specifically the unknown set.\n",
    "I found out that tho the whole dataset has an accuracy of 92%, the unknown set has only 45%. As a result, I sliced out the unknown set from the error set, and found that one of the biggest pattern was for hyphened words, which most of the time are JJ but almost always get predicted as NN. Combining with the lecture slide which mentioned pseudo-words, I added 3 more of those (mentioned in above section) and got a 60% for the unknown set. (From \"worse than guess\" to \"a good guess\")\n",
    "\n",
    "--Before my optimization using pseudo-words--\n",
    "\n",
    "What is the accuracy on the dev data?\n",
    "- 92.2%\n",
    "\n",
    "--After my optimization using pseudo-words--\n",
    "\n",
    "What is the accuracy on the dev data?\n",
    "- 94.1%"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.Viterbi Decoding with HMM\n",
    "\n",
    "--Before my optimization using pseudo-words--\n",
    "\n",
    "What is the accuracy on the dev data?\n",
    "- 93.4%\n",
    "\n",
    "--After my optimization using pseudo-words--\n",
    "\n",
    "What is the accuracy on the dev data?\n",
    "- 94.9%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0 (default, Nov 15 2020, 08:30:55) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ce56739b024e9597296b6666b65e03f262f75084e26dc2406f2b2f42b872b31f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
