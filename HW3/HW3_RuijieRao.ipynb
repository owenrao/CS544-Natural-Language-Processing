{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKZV0421pgHG"
      },
      "source": [
        "# CSCI544-Assignment 1\n",
        "-- by Ruijie Rao"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMl1rwz7sMrs",
        "outputId": "5d4cc600-e6ff-4aac-f6a7-3e698329a6dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ISKQ1ThxpgHJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import csv\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import gensim\n",
        "import pickle\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xWX1AoV-pgHK"
      },
      "outputs": [],
      "source": [
        "DATA_DIR = '/content/drive/MyDrive/USC/DS544/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3Jbpq4gpgHK"
      },
      "source": [
        "# Preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImIIwP4MU-_W"
      },
      "source": [
        "## Read and sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dutoGCh0pgHK",
        "outputId": "7918fc71-8c5e-4593-afec-453f2254027e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py:3326: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
            "\n",
            "\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        }
      ],
      "source": [
        "raw_df = pd.read_table(DATA_DIR+'amazon_reviews_us_Beauty_v1_00.tsv.gz', compression='gzip', quotechar='\"', error_bad_lines=False, quoting=csv.QUOTE_NONE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xnqro3UgpgHL"
      },
      "outputs": [],
      "source": [
        "df = raw_df[[\"star_rating\",\"review_body\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TDCZIDppgHL"
      },
      "outputs": [],
      "source": [
        "df = df.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvy2r63PpgHL"
      },
      "outputs": [],
      "source": [
        "def label_class(x):\n",
        "    if x<3:\n",
        "        return 0\n",
        "    if x>3:\n",
        "        return 2\n",
        "    else:\n",
        "        return 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCDjHQCppgHM"
      },
      "outputs": [],
      "source": [
        "df[\"label\"] = df[\"star_rating\"].apply(label_class)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ObF-2QazpgHM"
      },
      "outputs": [],
      "source": [
        "sampled_df = pd.concat([df[df[\"label\"] == k].sample(n=20000) for k in range(3)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKhEE5EtpgHM",
        "outputId": "9cfc09cc-b5c0-46b1-b279-1337621dbcdf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "60000"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(sampled_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "del raw_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKUfdHFlItQb"
      },
      "source": [
        "#### contraction dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXX_A1XJpgHM"
      },
      "outputs": [],
      "source": [
        "contractions = { \n",
        "    \"dont\":\"do not\",\n",
        "    \"ain't\": \"are not\",\n",
        "    \"aren't\": \"are not\",\n",
        "    \"can't\": \"cannot\",\n",
        "    \"can't've\": \"cannot have\",\n",
        "    \"'cause\": \"because\",\n",
        "    \"could've\": \"could have\",\n",
        "    \"couldn't\": \"could not\",\n",
        "    \"couldn't've\": \"could not have\",\n",
        "    \"didn't\": \"did not\",\n",
        "    \"doesn't\": \"does not\",\n",
        "    \"don't\": \"do not\",\n",
        "    \"hadn't\": \"had not\",\n",
        "    \"hadn't've\": \"had not have\",\n",
        "    \"hasn't\": \"has not\",\n",
        "    \"haven't\": \"have not\",\n",
        "    \"he'd\": \"he would\",\n",
        "    \"he'd've\": \"he would have\",\n",
        "    \"he'll\": \"he will\",\n",
        "    \"he'll've\": \"he will have\",\n",
        "    \"he's\": \"he is\",\n",
        "    \"how'd\": \"how did\",\n",
        "    \"how'd'y\": \"how do you\",\n",
        "    \"how'll\": \"how will\",\n",
        "    \"how's\": \"how iss\",\n",
        "    \"i'd\": \"i would\",\n",
        "    \"i'd've\": \"i would have\",\n",
        "    \"i'll\": \"i will\",\n",
        "    \"i'll've\": \"i will have\",\n",
        "    \"i'm\": \"i am\",\n",
        "    \"i've\": \"i have\",\n",
        "    \"isn't\": \"is not\",\n",
        "    \"it'd\": \"it would\",\n",
        "    \"it'd've\": \"it would have\",\n",
        "    \"it'll\": \"it will\",\n",
        "    \"it'll've\": \"it will have\",\n",
        "    \"it's\": \"it is\",\n",
        "    \"let's\": \"let us\",\n",
        "    \"ma'am\": \"madam\",\n",
        "    \"mayn't\": \"may not\",\n",
        "    \"might've\": \"might have\",\n",
        "    \"mightn't\": \"might not\",\n",
        "    \"mightn't've\": \"might not have\",\n",
        "    \"must've\": \"must have\",\n",
        "    \"mustn't\": \"must not\",\n",
        "    \"mustn't've\": \"must not have\",\n",
        "    \"needn't\": \"need not\",\n",
        "    \"needn't've\": \"need not have\",\n",
        "    \"o'clock\": \"of the clock\",\n",
        "    \"oughtn't\": \"ought not\",\n",
        "    \"oughtn't've\": \"ought not have\",\n",
        "    \"shan't\": \"shall not\",\n",
        "    \"sha'n't\": \"shall not\",\n",
        "    \"shan't've\": \"shall not have\",\n",
        "    \"she'd\": \"she would\",\n",
        "    \"she'd've\": \"she would have\",\n",
        "    \"she'll\": \"she will\",\n",
        "    \"she'll've\": \"she will have\",\n",
        "    \"she's\": \"she is\",\n",
        "    \"should've\": \"should have\",\n",
        "    \"shouldn't\": \"should not\",\n",
        "    \"shouldn't've\": \"should not have\",\n",
        "    \"so've\": \"so have\",\n",
        "    \"so's\": \"so is\",\n",
        "    \"that'd\": \"that would\",\n",
        "    \"that'd've\": \"that would have\",\n",
        "    \"that's\": \"that is\",\n",
        "    \"there'd\": \"there would\",\n",
        "    \"there'd've\": \"there would have\",\n",
        "    \"there's\": \"there is\",\n",
        "    \"they'd\": \"they would\",\n",
        "    \"they'd've\": \"they would have\",\n",
        "    \"they'll\": \"they will\",\n",
        "    \"they'll've\": \"they will have\",\n",
        "    \"they're\": \"they are\",\n",
        "    \"they've\": \"they have\",\n",
        "    \"to've\": \"to have\",\n",
        "    \"wasn't\": \"was not\",\n",
        "    \"we'd\": \"we would\",\n",
        "    \"we'd've\": \"we would have\",\n",
        "    \"we'll\": \"we will\",\n",
        "    \"we'll've\": \"we will have\",\n",
        "    \"we're\": \"we are\",\n",
        "    \"we've\": \"we have\",\n",
        "    \"weren't\": \"were not\",\n",
        "    \"what'll\": \"what will\",\n",
        "    \"what'll've\": \"what will have\",\n",
        "    \"what're\": \"what are\",\n",
        "    \"what's\": \"what is\",\n",
        "    \"what've\": \"what have\",\n",
        "    \"when's\": \"when is\",\n",
        "    \"when've\": \"when have\",\n",
        "    \"where'd\": \"where did\",\n",
        "    \"where's\": \"where is\",\n",
        "    \"where've\": \"where have\",\n",
        "    \"who'll\": \"who will\",\n",
        "    \"who'll've\": \"who will have\",\n",
        "    \"who's\": \"who is\",\n",
        "    \"who've\": \"who have\",\n",
        "    \"why's\": \"why is\",\n",
        "    \"why've\": \"why have\",\n",
        "    \"will've\": \"will have\",\n",
        "    \"won't\": \"will not\",\n",
        "    \"won't've\": \"will not have\",\n",
        "    \"would've\": \"would have\",\n",
        "    \"wouldn't\": \"would not\",\n",
        "    \"wouldn't've\": \"would not have\",\n",
        "    \"y'all\": \"you all\",\n",
        "    \"y'all'd\": \"you all would\",\n",
        "    \"y'all'd've\": \"you all would have\",\n",
        "    \"y'all're\": \"you all are\",\n",
        "    \"y'all've\": \"you all have\",\n",
        "    \"you'd\": \"you would\",\n",
        "    \"you'd've\": \"you would have\",\n",
        "    \"you'll\": \"yyou will\",\n",
        "    \"you'll've\": \"you shall have \",\n",
        "    \"you're\": \"you are\",\n",
        "    \"you've\": \"you have\",\n",
        "    \"aint\": \"are not\",\n",
        "    \"arent\": \"are not\",\n",
        "    \"cant\": \"cannot\",\n",
        "    \"cantve\": \"cannot have\",\n",
        "    \"cause\": \"because\",\n",
        "    \"couldve\": \"could have\",\n",
        "    \"couldnt\": \"could not\",\n",
        "    \"couldntve\": \"could not have\",\n",
        "    \"didnt\": \"did not\",\n",
        "    \"doesnt\": \"does not\",\n",
        "    \"dont\": \"do not\",\n",
        "    \"hadnt\": \"had not\",\n",
        "    \"hadntve\": \"had not have\",\n",
        "    \"hasnt\": \"has not\",\n",
        "    \"havent\": \"have not\",\n",
        "    \"hed\": \"he would\",\n",
        "    \"hedve\": \"he would have\",\n",
        "    \"hell\": \"he will\",\n",
        "    \"hellve\": \"he will have\",\n",
        "    \"hes\": \"he is\",\n",
        "    \"howd\": \"how did\",\n",
        "    \"howdy\": \"how do you\",\n",
        "    \"howll\": \"how will\",\n",
        "    \"hows\": \"how iss\",\n",
        "    \"id\": \"i would\",\n",
        "    \"idve\": \"i would have\",\n",
        "    \"ill\": \"i will\",\n",
        "    \"illve\": \"i will have\",\n",
        "    \"im\": \"i am\",\n",
        "    \"ive\": \"i have\",\n",
        "    \"isnt\": \"is not\",\n",
        "    \"itd\": \"it would\",\n",
        "    \"itdve\": \"it would have\",\n",
        "    \"itll\": \"it will\",\n",
        "    \"itllve\": \"it will have\",\n",
        "    \"its\": \"it is\",\n",
        "    \"lets\": \"let us\",\n",
        "    \"maam\": \"madam\",\n",
        "    \"maynt\": \"may not\",\n",
        "    \"mightve\": \"might have\",\n",
        "    \"mightnt\": \"might not\",\n",
        "    \"mightntve\": \"might not have\",\n",
        "    \"mustve\": \"must have\",\n",
        "    \"mustnt\": \"must not\",\n",
        "    \"mustntve\": \"must not have\",\n",
        "    \"neednt\": \"need not\",\n",
        "    \"needntve\": \"need not have\",\n",
        "    \"oclock\": \"of the clock\",\n",
        "    \"oughtnt\": \"ought not\",\n",
        "    \"oughtntve\": \"ought not have\",\n",
        "    \"shant\": \"shall not\",\n",
        "    \"shant\": \"shall not\",\n",
        "    \"shantve\": \"shall not have\",\n",
        "    \"shed\": \"she would\",\n",
        "    \"shedve\": \"she would have\",\n",
        "    \"shell\": \"she will\",\n",
        "    \"shellve\": \"she will have\",\n",
        "    \"shes\": \"she is\",\n",
        "    \"shouldve\": \"should have\",\n",
        "    \"shouldnt\": \"should not\",\n",
        "    \"shouldntve\": \"should not have\",\n",
        "    \"sove\": \"so have\",\n",
        "    \"sos\": \"so is\",\n",
        "    \"thatd\": \"that would\",\n",
        "    \"thatdve\": \"that would have\",\n",
        "    \"thats\": \"that is\",\n",
        "    \"thered\": \"there would\",\n",
        "    \"theredve\": \"there would have\",\n",
        "    \"theres\": \"there is\",\n",
        "    \"theyd\": \"they would\",\n",
        "    \"theydve\": \"they would have\",\n",
        "    \"theyll\": \"they will\",\n",
        "    \"theyllve\": \"they will have\",\n",
        "    \"theyre\": \"they are\",\n",
        "    \"theyve\": \"they have\",\n",
        "    \"tove\": \"to have\",\n",
        "    \"wasnt\": \"was not\",\n",
        "    \"wed\": \"we would\",\n",
        "    \"wedve\": \"we would have\",\n",
        "    \"well\": \"we will\",\n",
        "    \"wellve\": \"we will have\",\n",
        "    \"were\": \"we are\",\n",
        "    \"weve\": \"we have\",\n",
        "    \"werent\": \"were not\",\n",
        "    \"whatll\": \"what will\",\n",
        "    \"whatllve\": \"what will have\",\n",
        "    \"whatre\": \"what are\",\n",
        "    \"whats\": \"what is\",\n",
        "    \"whatve\": \"what have\",\n",
        "    \"whens\": \"when is\",\n",
        "    \"whenve\": \"when have\",\n",
        "    \"whered\": \"where did\",\n",
        "    \"wheres\": \"where is\",\n",
        "    \"whereve\": \"where have\",\n",
        "    \"wholl\": \"who will\",\n",
        "    \"whollve\": \"who will have\",\n",
        "    \"whos\": \"who is\",\n",
        "    \"whove\": \"who have\",\n",
        "    \"whys\": \"why is\",\n",
        "    \"whyve\": \"why have\",\n",
        "    \"willve\": \"will have\",\n",
        "    \"wont\": \"will not\",\n",
        "    \"wontve\": \"will not have\",\n",
        "    \"wouldve\": \"would have\",\n",
        "    \"wouldnt\": \"would not\",\n",
        "    \"wouldntve\": \"would not have\",\n",
        "    \"yall\": \"you all\",\n",
        "    \"yalld\": \"you all would\",\n",
        "    \"yalldve\": \"you all would have\",\n",
        "    \"yallre\": \"you all are\",\n",
        "    \"yallve\": \"you all have\",\n",
        "    \"youd\": \"you would\",\n",
        "    \"youdve\": \"you would have\",\n",
        "    \"youll\": \"yyou will\",\n",
        "    \"youllve\": \"you shall have \",\n",
        "    \"youre\": \"you are\",\n",
        "    \"youve\": \"you have\",\n",
        "    \"ain t\": \"are not\",\n",
        "    \"aren t\": \"are not\",\n",
        "    \"can t\": \"cannot\",\n",
        "    \"can t ve\": \"cannot have\",\n",
        "    \" cause\": \"because\",\n",
        "    \"could ve\": \"could have\",\n",
        "    \"couldn t\": \"could not\",\n",
        "    \"couldn t ve\": \"could not have\",\n",
        "    \"didn t\": \"did not\",\n",
        "    \"doesn t\": \"does not\",\n",
        "    \"don t\": \"do not\",\n",
        "    \"hadn t\": \"had not\",\n",
        "    \"hadn t ve\": \"had not have\",\n",
        "    \"hasn t\": \"has not\",\n",
        "    \"haven t\": \"have not\",\n",
        "    \"he d\": \"he would\",\n",
        "    \"he d ve\": \"he would have\",\n",
        "    \"he ll\": \"he will\",\n",
        "    \"he ll ve\": \"he will have\",\n",
        "    \"he s\": \"he is\",\n",
        "    \"how d\": \"how did\",\n",
        "    \"how d y\": \"how do you\",\n",
        "    \"how ll\": \"how will\",\n",
        "    \"how s\": \"how iss\",\n",
        "    \"i d\": \"i would\",\n",
        "    \"i d ve\": \"i would have\",\n",
        "    \"i ll\": \"i will\",\n",
        "    \"i ll ve\": \"i will have\",\n",
        "    \"i m\": \"i am\",\n",
        "    \"i ve\": \"i have\",\n",
        "    \"isn t\": \"is not\",\n",
        "    \"it d\": \"it would\",\n",
        "    \"it d ve\": \"it would have\",\n",
        "    \"it ll\": \"it will\",\n",
        "    \"it ll ve\": \"it will have\",\n",
        "    \"it s\": \"it is\",\n",
        "    \"let s\": \"let us\",\n",
        "    \"ma am\": \"madam\",\n",
        "    \"mayn t\": \"may not\",\n",
        "    \"might ve\": \"might have\",\n",
        "    \"mightn t\": \"might not\",\n",
        "    \"mightn t ve\": \"might not have\",\n",
        "    \"must ve\": \"must have\",\n",
        "    \"mustn t\": \"must not\",\n",
        "    \"mustn t ve\": \"must not have\",\n",
        "    \"needn t\": \"need not\",\n",
        "    \"needn t ve\": \"need not have\",\n",
        "    \"o clock\": \"of the clock\",\n",
        "    \"oughtn t\": \"ought not\",\n",
        "    \"oughtn t ve\": \"ought not have\",\n",
        "    \"shan t\": \"shall not\",\n",
        "    \"sha n t\": \"shall not\",\n",
        "    \"shan t ve\": \"shall not have\",\n",
        "    \"she d\": \"she would\",\n",
        "    \"she d ve\": \"she would have\",\n",
        "    \"she ll\": \"she will\",\n",
        "    \"she ll ve\": \"she will have\",\n",
        "    \"she s\": \"she is\",\n",
        "    \"should ve\": \"should have\",\n",
        "    \"shouldn t\": \"should not\",\n",
        "    \"shouldn t ve\": \"should not have\",\n",
        "    \"so ve\": \"so have\",\n",
        "    \"so s\": \"so is\",\n",
        "    \"that d\": \"that would\",\n",
        "    \"that d ve\": \"that would have\",\n",
        "    \"that s\": \"that is\",\n",
        "    \"there d\": \"there would\",\n",
        "    \"there d ve\": \"there would have\",\n",
        "    \"there s\": \"there is\",\n",
        "    \"they d\": \"they would\",\n",
        "    \"they d ve\": \"they would have\",\n",
        "    \"they ll\": \"they will\",\n",
        "    \"they ll ve\": \"they will have\",\n",
        "    \"they re\": \"they are\",\n",
        "    \"they ve\": \"they have\",\n",
        "    \"to ve\": \"to have\",\n",
        "    \"wasn t\": \"was not\",\n",
        "    \"we d\": \"we would\",\n",
        "    \"we d ve\": \"we would have\",\n",
        "    \"we ll\": \"we will\",\n",
        "    \"we ll ve\": \"we will have\",\n",
        "    \"we re\": \"we are\",\n",
        "    \"we ve\": \"we have\",\n",
        "    \"weren t\": \"were not\",\n",
        "    \"what ll\": \"what will\",\n",
        "    \"what ll ve\": \"what will have\",\n",
        "    \"what re\": \"what are\",\n",
        "    \"what s\": \"what is\",\n",
        "    \"what ve\": \"what have\",\n",
        "    \"when s\": \"when is\",\n",
        "    \"when ve\": \"when have\",\n",
        "    \"where d\": \"where did\",\n",
        "    \"where s\": \"where is\",\n",
        "    \"where ve\": \"where have\",\n",
        "    \"who ll\": \"who will\",\n",
        "    \"who ll ve\": \"who will have\",\n",
        "    \"who s\": \"who is\",\n",
        "    \"who ve\": \"who have\",\n",
        "    \"why s\": \"why is\",\n",
        "    \"why ve\": \"why have\",\n",
        "    \"will ve\": \"will have\",\n",
        "    \"won t\": \"will not\",\n",
        "    \"won t ve\": \"will not have\",\n",
        "    \"would ve\": \"would have\",\n",
        "    \"wouldn t\": \"would not\",\n",
        "    \"wouldn t ve\": \"would not have\",\n",
        "    \"y all\": \"you all\",\n",
        "    \"y all d\": \"you all would\",\n",
        "    \"y all d ve\": \"you all would have\",\n",
        "    \"y all re\": \"you all are\",\n",
        "    \"y all ve\": \"you all have\",\n",
        "    \"you d\": \"you would\",\n",
        "    \"you d ve\": \"you would have\",\n",
        "    \"you ll\": \"yyou will\",\n",
        "    \"you ll ve\": \"you shall have \",\n",
        "    \"you re\": \"you are\",\n",
        "    \"you ve\": \"you have\"\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cm_j88RnIxhE"
      },
      "source": [
        "## decontract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFIkPOcipgHO"
      },
      "outputs": [],
      "source": [
        "def decontract(x):\n",
        "    tokens = x.split(' ')\n",
        "    for i,token in enumerate(tokens):\n",
        "        if token in contractions.keys():\n",
        "            tokens[i] = contractions[token]\n",
        "    return ' '.join(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBlMVpPcHWi4"
      },
      "source": [
        "## cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YB7ts1tipgHO"
      },
      "outputs": [],
      "source": [
        "def data_cleaning(x):\n",
        "    my_stopwords = [\"a\", \"and\", \"of\", \"to\"]\n",
        "    x = x.lower() #convert all reviews into lowercase\n",
        "    x = re.sub(r'<br />', '', x)\n",
        "    x = re.sub(r'\\s*https?://\\S+(\\s+|$)', '', x) #remove the HTML and URLs from the reviews\n",
        "    x = re.sub(r'[^a-zA-Z ]+', ' ', x) #remove non-alphabetical characters\n",
        "    x = ' '.join(re.sub(r'\\s', ' ', x).split()) #remove extra spaces\n",
        "    x = ' '.join([word for word in x.split(\" \") if word not in my_stopwords])\n",
        "    x = decontract(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-kPXRSY6pgHO"
      },
      "outputs": [],
      "source": [
        "sampled_df[\"review_cleaned\"] = sampled_df[\"review_body\"].apply(data_cleaning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "_OgaUf3eR6Gz"
      },
      "outputs": [],
      "source": [
        "with open(DATA_DIR+\"sampled_df.pkl\",\"wb\") as file:\n",
        "    pickle.dump(sampled_df, file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iJ72zukOgKv"
      },
      "source": [
        "## create bag of words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDsFmWilOQBe"
      },
      "outputs": [],
      "source": [
        "bow = {}\n",
        "def gen_bow(x,bow):\n",
        "  for word in x.split(\" \"):\n",
        "    try:\n",
        "      bow[word] += 1\n",
        "    except:\n",
        "      bow[word] = 1\n",
        "  return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QrXz2U8PJo4"
      },
      "outputs": [],
      "source": [
        "sampled_df[\"review_cleaned\"].apply(lambda x: gen_bow(x,bow))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lunEBX6aPUi8"
      },
      "outputs": [],
      "source": [
        "with open(DATA_DIR+\"bow.pkl\",\"wb\") as file:\n",
        "    pickle.dump(bow, file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCcptNuNpgHQ"
      },
      "source": [
        "# Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TxgFkwC3pgHQ"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader as api\n",
        "wv = api.load('word2vec-google-news-300',)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "S4ql0xHdJxz0"
      },
      "outputs": [],
      "source": [
        "gg_model = gensim.models.KeyedVectors.load_word2vec_format(DATA_DIR+\"gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz\", binary=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OftfBLx1L8eF"
      },
      "source": [
        "## Check coverage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t74FQtyKL_EG"
      },
      "outputs": [],
      "source": [
        "def check_coverage(wv, bow):\n",
        "  covered = {}\n",
        "  uncovered = {}\n",
        "  for word, count in bow.items():\n",
        "    try: # found in wv\n",
        "      temp = wv[word]\n",
        "      covered[word] = bow[word]\n",
        "    except: # if not found in wv\n",
        "      uncovered[word] = bow[word]\n",
        "  print('Found embeddings for {:.2%} of vocab'.format(len(covered) / len(bow)))\n",
        "  print('Found embeddings for  {:.2%} of all text'.format(sum(covered.values())/ sum(bow.values())))\n",
        "  result = sorted(uncovered.items(), key=lambda x: x[1])[::-1]\n",
        "\n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fihZZoMPN95a"
      },
      "outputs": [],
      "source": [
        "with open(DATA_DIR+\"bow.pkl\",\"rb\") as file:\n",
        "    bow = pickle.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjQcmdZrQaa7",
        "outputId": "dc333f4a-0c5d-4cf5-c24f-9c0d9406f56d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found embeddings for 47.71% of vocab\n",
            "Found embeddings for  98.35% of all text\n"
          ]
        }
      ],
      "source": [
        "uncovered = check_coverage(gg_model, bow)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RK6b3NqGUhh5"
      },
      "source": [
        "## Check semantic similarities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_sW1E3vVm2k"
      },
      "outputs": [],
      "source": [
        "def compare_difference(w,v):\n",
        "  return np.sqrt(sum(np.square(w-v)))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cTLPdnFPWAx3"
      },
      "source": [
        "#### `\"Cat\", \"Car\", and \"Bus\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtDVCkpWUxxg",
        "outputId": "156a569f-c6d1-4355-84e8-c57eb172ece5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3.5739133148105315"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "compare_difference(gg_model[\"cat\"],gg_model[\"car\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eX6uJMauWGg2",
        "outputId": "de89ef60-0127-483e-a014-9cd324f0b868"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2.9155472320880684"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "compare_difference(gg_model[\"bus\"],gg_model[\"car\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rkFOAEMWmtW"
      },
      "source": [
        "#### King − Man + Woman = Queen "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9kdwrFkWvBp",
        "outputId": "daae2845-ca8e-4239-8006-9dbbea38dd1a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2.298657801924729"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pred = gg_model[\"king\"]-gg_model[\"man\"]+gg_model[\"woman\"]\n",
        "compare_difference(pred,gg_model[\"queen\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLJruWYkXAvn",
        "outputId": "14279118-c448-44b2-a139-6c163ad6087b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2.479692373238762"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "compare_difference(gg_model[\"queen\"],gg_model[\"king\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTu8geHRX7Mu"
      },
      "source": [
        "#### excellent ∼ outstanding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_EvSnHDX9lE",
        "outputId": "4403de56-8572-4f99-e17c-a566ac0f0a7f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2.4881580884687127"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "compare_difference(gg_model[\"excellent\"],gg_model[\"outstanding\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuweUVoeYFhK",
        "outputId": "ba8bcdd2-272b-4c66-9015-b71ea714c90b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2.957787185080837"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "compare_difference(gg_model[\"excellent\"],gg_model[\"terrible\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kx2vpq4ZYMhF"
      },
      "source": [
        "## Build my own Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUu9_V_0aNNg"
      },
      "outputs": [],
      "source": [
        "from gensim import utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JnL1pxuLaEZK"
      },
      "outputs": [],
      "source": [
        "sentences = sampled_df[\"review_cleaned\"].apply(lambda x: x.split()).values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9SFN7g0CajsY"
      },
      "outputs": [],
      "source": [
        "class MyCorpus:\n",
        "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
        "    def __init__(self, sentences):\n",
        "      self.sentences = sentences\n",
        "\n",
        "    def __iter__(self):\n",
        "        for line in self.sentences:\n",
        "            # assume there's one document per line, tokens separated by whitespace\n",
        "            yield line"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XlPgGE_3bJyn"
      },
      "outputs": [],
      "source": [
        "corpus = MyCorpus(sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSM96VKoYRlU"
      },
      "outputs": [],
      "source": [
        "my_model = gensim.models.Word2Vec(sentences=corpus, min_count=9, size=300, window=13)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SOJHQsXdspK"
      },
      "source": [
        "## Check semantic similarities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmHvn4fedspM"
      },
      "source": [
        "#### King − Man + Woman = Queen "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65erL5yWdspM",
        "outputId": "4bccc93f-d83d-449a-e880-fddb8bd851d6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-79-c0baaff80348>:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  pred = my_model[\"king\"]-my_model[\"man\"]+my_model[\"woman\"]\n",
            "<ipython-input-79-c0baaff80348>:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  compare_difference(pred,my_model[\"queen\"])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "4.871180486922247"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pred = my_model[\"king\"]-my_model[\"man\"]+my_model[\"woman\"]\n",
        "compare_difference(pred,my_model[\"queen\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSLAflVWdspM",
        "outputId": "eebe61bc-9471-4a81-89ab-3e64d4346140"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-80-5fe7239160ae>:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  compare_difference(my_model[\"queen\"],my_model[\"king\"])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "1.5191899898462256"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "compare_difference(my_model[\"queen\"],my_model[\"king\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u16VDlvSdspM"
      },
      "source": [
        "#### excellent ∼ outstanding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xoxn-2rRdspM",
        "outputId": "2bf243f7-6a7a-4fd2-d260-5324a164e429"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-81-d0bd3f31e48d>:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  compare_difference(my_model[\"excellent\"],my_model[\"outstanding\"])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "10.07535627275851"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "compare_difference(my_model[\"excellent\"],my_model[\"outstanding\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3RqfJytdspM",
        "outputId": "2fbc15b3-cfcf-46d9-e8c9-34e25a62aa32"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-82-d357210032c4>:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  compare_difference(my_model[\"excellent\"],my_model[\"terrible\"])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "11.696851275335957"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "compare_difference(my_model[\"excellent\"],my_model[\"terrible\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqR0frD-e3tX"
      },
      "source": [
        "## Conclusion\n",
        "- In the King and Queen test, my model does terribly since the \"made up queen\" has a bigger distance from the actually queen than king itself.\n",
        "- In the excellent ∼ outstanding test, the result is also not satisfying, though atleast excellent and outstanding is more similar than exceleent and terrible.\n",
        "- To conclude, it is obvious that the google news pretrained model performs much better and more aligned with common sense. I do believe this is due to its huge corpus and difference in context. Google news definitely contains more semantic information than limited amount of product reviews."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gIBpkGN-se4"
      },
      "source": [
        "# Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "SIWV-lqfBHAn"
      },
      "outputs": [],
      "source": [
        "def gen_mean_embed(x, wv):\n",
        "  shp = (1,300)\n",
        "  result = []\n",
        "  tokens = x.split(\" \")\n",
        "  count = 0\n",
        "  for word in tokens:\n",
        "    try:\n",
        "      result.append(wv[word].reshape(shp))\n",
        "      count += 1\n",
        "    except:\n",
        "      continue\n",
        "  if len(result) == 0:\n",
        "    return np.zeros(shp).astype('float32')\n",
        "  return np.mean(result, axis=0).astype('float32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "JDTC7lIKs4Hu"
      },
      "outputs": [],
      "source": [
        "def gen_cat_embed(x, wv, max_len=10):\n",
        "  shp = wv[\"king\"].shape\n",
        "  result = []\n",
        "  tokens = x.split(\" \")\n",
        "  count = 0\n",
        "  for word in tokens:\n",
        "    try:\n",
        "      result.append(wv[word].reshape(shp))\n",
        "      count += 1\n",
        "    except:\n",
        "      continue\n",
        "    if count==max_len:\n",
        "      break\n",
        "  if len(result) < max_len:\n",
        "    pad_len = max_len-len(result)\n",
        "    result += [np.zeros(shp) for i in range(pad_len)]\n",
        "  return np.array(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ihVPfEn4FEM1"
      },
      "outputs": [],
      "source": [
        "sampled_df[\"mean_emb\"] = sampled_df[\"review_body\"].apply(lambda x: gen_mean_embed(x, gg_model))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "lUIwJ1fduGA1"
      },
      "outputs": [],
      "source": [
        "sampled_df[\"cat_emb\"] = sampled_df[\"review_body\"].apply(lambda x: gen_cat_embed(x, gg_model))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "B8k0KSieP6mP"
      },
      "outputs": [],
      "source": [
        "sampled_df[\"rnn_emb\"] = sampled_df[\"review_body\"].apply(lambda x: gen_cat_embed(x, gg_model, 20))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "del gg_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "SBGhbK_AF3vH"
      },
      "outputs": [],
      "source": [
        "class Word2Vec(Dataset):\n",
        "    \n",
        "  def __init__(self, df, transform=None):\n",
        "      self.data = df\n",
        "      self.transform = transform\n",
        "      \n",
        "  def __len__(self):\n",
        "      return len(self.data)\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "      emb = self.data.iloc[index,0].astype('float32') #.reshape(())\n",
        "      label = self.data.iloc[index,1]\n",
        "      \n",
        "      if self.transform is not None:\n",
        "          image = self.transform(emb)\n",
        "          \n",
        "      return emb, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Co7Fc9JKGI2Q"
      },
      "outputs": [],
      "source": [
        "def gen_train_test_loader(feature_df, batch_size=64):\n",
        "  data = Word2Vec(feature_df, transform=transforms.ToTensor())\n",
        "  train, val, test = random_split(data,[int(np.floor(len(feature_df)*0.6)),int(np.floor(len(feature_df)*0.2)),int(np.floor(len(feature_df)*0.2))])\n",
        "  train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size,\n",
        "    shuffle=True)\n",
        "  val_loader = torch.utils.data.DataLoader(val, batch_size=batch_size,\n",
        "    shuffle=False)\n",
        "  test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, \n",
        "      shuffle=False)\n",
        "  return train_loader, val_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "YApE92K5-xcG"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(sampled_df[\"mean_emb\"], sampled_df[['label']], test_size=0.2, random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQb94SJxpgHQ"
      },
      "source": [
        "# Perceptron"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "skJTQxjwpgHQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
        "def evaluation(y_pred, y_true):\n",
        "    prc = precision_score(y_true, y_pred, average=None)\n",
        "    recall = recall_score(y_true, y_pred, average=None)\n",
        "    f1 = f1_score(y_true, y_pred, average=None)\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    for cls in range(1,4):\n",
        "        print(f'<Class {cls}> Precision: {prc[cls-1]}, Recall: {recall[cls-1]}, F-1: {f1[cls-1]}')\n",
        "    print(f'<Overall Mean> Precision: {np.mean(prc)}, Recall: {np.mean(recall)}, F-1: {np.mean(f1)}, Accuracy: {np.mean(acc)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Iaj_18nBpgHQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Perceptron"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdZwXYlKpgHR",
        "outputId": "d1825252-a561-456b-8d8d-62c84a11c832"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<Class 1> Precision: 0.536787040161998, Recall: 0.7958468851638729, F-1: 0.6411367529980853\n",
            "<Class 2> Precision: 0.5492424242424242, Recall: 0.5388999008919723, F-1: 0.5440220110055027\n",
            "<Class 3> Precision: 0.8935666982024598, Recall: 0.47617847239727756, F-1: 0.6212793948363756\n",
            "<Overall Mean> Precision: 0.6598653875356274, Recall: 0.6036417528177076, F-1: 0.6021460529466545, Accuracy: 0.60375\n"
          ]
        }
      ],
      "source": [
        "#best_seed = find_best_randseed(20)\n",
        "perceptron_md = Perceptron(tol=1e-3, random_state=0)\n",
        "perceptron_md.fit(np.stack(X_train), y_train.values.ravel())\n",
        "evaluation(perceptron_md.predict(np.stack(X_test)), y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FB_JYmGypgHR"
      },
      "source": [
        "# SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "VFGrU4m5pgHR"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC, LinearSVC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "My2pkSCqpgHR",
        "outputId": "7ce686f0-9dba-4c96-ebed-5372c38dbea5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<Class 1> Precision: 0.6834581347855684, Recall: 0.7535651738804103, F-1: 0.7168015230842456\n",
            "<Class 2> Precision: 0.6408713098308971, Recall: 0.554013875123885, F-1: 0.5942857142857142\n",
            "<Class 3> Precision: 0.7863060428849903, Recall: 0.813461053692967, F-1: 0.7996530789245447\n",
            "<Overall Mean> Precision: 0.7035451625004853, Recall: 0.7070133675657541, F-1: 0.7035801054315015, Accuracy: 0.70625\n"
          ]
        }
      ],
      "source": [
        "svm_md = LinearSVC(random_state=0, dual=False, C=0.05)\n",
        "svm_md.fit(np.stack(X_train), y_train.values.ravel())\n",
        "evaluation(svm_md.predict(np.stack(X_test)), y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vc6bHYW8jZu_"
      },
      "source": [
        "# Train and Eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "LhF1FR0dGarx"
      },
      "outputs": [],
      "source": [
        "def eval(model, loader, criterion):\n",
        "  test_loss = 0.0\n",
        "  correct_count = 0\n",
        "  model.eval()\n",
        "  for data, label in loader:\n",
        "    data.to(device)\n",
        "    label.to(device)\n",
        "    pred = model(data)\n",
        "    loss = criterion(pred, label)\n",
        "    test_loss += loss.item()*data.size(0)\n",
        "    correct_count += (pred.argmax(axis=1) == label).sum().item() \n",
        "  test_loss = test_loss/len(loader.dataset)\n",
        "  test_acc = correct_count/len(loader.dataset)\n",
        "  return test_loss, test_acc\n",
        "\n",
        "def train(model, train_loader, val_loader, test_loader, epoch_num, lr=0.01):\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  #optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "  weight_decay = 1e-4\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "  best_val_loss = np.inf\n",
        "  for epoch in range(epoch_num):\n",
        "    train_loss = 0.0\n",
        "    model.train()\n",
        "    for data, label in train_loader:\n",
        "      data.to(device)\n",
        "      label.to(device)\n",
        "      label = F.one_hot(label).float()\n",
        "      optimizer.zero_grad()\n",
        "      pred = model(data)\n",
        "      #print(pred.shape,label.shape)\n",
        "      loss = criterion(pred, label)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      train_loss += loss.item()*data.size(0)\n",
        "    train_loss = train_loss/len(train_loader.dataset)\n",
        "    val_loss, val_acc = eval(model, val_loader, criterion)\n",
        "    #if epoch_num>10 and (epoch+1)%10 == 0:\n",
        "    print(f\"[Epoch {epoch+1}] Train Loss: {train_loss}\")\n",
        "    print(f\"[Epoch {epoch+1}] Val Loss: {val_loss}\")\n",
        "    if val_loss<best_val_loss:\n",
        "      best_val_loss = val_loss\n",
        "      torch.save({\n",
        "            'epoch': epoch+1,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': criterion,\n",
        "            }, DATA_DIR+'best_model.pth')\n",
        "  #print(best_val_loss)\n",
        "  checkpoint = torch.load(DATA_DIR+'best_model.pth')\n",
        "  model.load_state_dict(checkpoint['model_state_dict'])\n",
        "  test_loss, test_acc = eval(model, test_loader, criterion)\n",
        "  \n",
        "  print(f\"Test Loss: {test_loss}\")\n",
        "  print(f\"Test Accuracy: {test_acc}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7osNESvZ_VXb"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "UrqJ0i12enI5"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRKOijUrMz7K"
      },
      "source": [
        "# FNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "TMNPpp6UM214"
      },
      "outputs": [],
      "source": [
        "class ForwardNeuralNetwork(nn.Module):\n",
        "  def __init__(self, input_size=300, dropout=0.2):\n",
        "    super(ForwardNeuralNetwork, self).__init__()\n",
        "    hidden_1 = 100\n",
        "    hidden_2 = 10\n",
        "    self.input_size = input_size\n",
        "    self.layer1 = nn.Linear(input_size, hidden_1)\n",
        "    self.layer2 = nn.Linear(hidden_1, hidden_2)\n",
        "    self.layer3 = nn.Linear(hidden_2, 3)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = x.view(-1, self.input_size)\n",
        "    x = F.relu(self.layer1(x))\n",
        "    x = self.dropout(x)\n",
        "    x = F.relu(self.layer2(x))\n",
        "    x = self.dropout(x)\n",
        "    x = self.layer3(x)\n",
        "    return F.log_softmax(x, dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4T8LZIevHfu"
      },
      "source": [
        "## Mean Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "JGaiY89jKwDZ"
      },
      "outputs": [],
      "source": [
        "train_loader, val_loader, test_loader = gen_train_test_loader(sampled_df[[\"mean_emb\",\"label\"]].sample(60000), batch_size=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhjfhbzbLHpr",
        "outputId": "e6987206-1812-41c4-f4b6-b79316c22983"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 1] Train Loss: 0.8137290114296807\n",
            "[Epoch 1] Val Loss: 0.7110626417795817\n",
            "[Epoch 2] Train Loss: 0.7230893446604411\n",
            "[Epoch 2] Val Loss: 0.6878786160151164\n",
            "[Epoch 3] Train Loss: 0.7082717269791498\n",
            "[Epoch 3] Val Loss: 0.6764097997347513\n",
            "[Epoch 4] Train Loss: 0.6940510704252455\n",
            "[Epoch 4] Val Loss: 0.6754676510492961\n",
            "[Epoch 5] Train Loss: 0.6834821257591247\n",
            "[Epoch 5] Val Loss: 0.6729069339434306\n",
            "[Epoch 6] Train Loss: 0.68030215660731\n",
            "[Epoch 6] Val Loss: 0.6731184935569763\n",
            "[Epoch 7] Train Loss: 0.6698232576052348\n",
            "[Epoch 7] Val Loss: 0.6558108727137247\n",
            "[Epoch 8] Train Loss: 0.6691948196093241\n",
            "[Epoch 8] Val Loss: 0.655287299156189\n",
            "[Epoch 9] Train Loss: 0.6626195336447822\n",
            "[Epoch 9] Val Loss: 0.6526960207621256\n",
            "[Epoch 10] Train Loss: 0.6559046327273051\n",
            "[Epoch 10] Val Loss: 0.6530044225056966\n",
            "0.6526960207621256\n",
            "Test Loss: 0.6445308755238851\n",
            "Test Accuracy: 0.71875\n"
          ]
        }
      ],
      "source": [
        "fnn = ForwardNeuralNetwork(dropout=0.2).to(device)\n",
        "train(fnn, train_loader, val_loader, test_loader, epoch_num=10, lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oV-8LR0vLbI"
      },
      "source": [
        "## Concat Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "BmJDjrEQhuNg"
      },
      "outputs": [],
      "source": [
        "train_loader, val_loader, test_loader = gen_train_test_loader(sampled_df[[\"cat_emb\",\"label\"]].sample(60000), batch_size=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vy8MS65XwEsV",
        "outputId": "acc9349a-8380-4c68-f545-03dc601df9f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 1] Train Loss: 0.8086624857584636\n",
            "[Epoch 1] Val Loss: 0.7154063116709392\n",
            "[Epoch 2] Train Loss: 0.7198042540550232\n",
            "[Epoch 2] Val Loss: 0.6951276100476583\n",
            "[Epoch 3] Train Loss: 0.6868773303031921\n",
            "[Epoch 3] Val Loss: 0.6905115958849589\n",
            "[Epoch 4] Train Loss: 0.6580080616209242\n",
            "[Epoch 4] Val Loss: 0.699057316939036\n",
            "[Epoch 5] Train Loss: 0.6290875034862095\n",
            "[Epoch 5] Val Loss: 0.6917047092119852\n",
            "[Epoch 6] Train Loss: 0.6017075389226277\n",
            "[Epoch 6] Val Loss: 0.6940470565954844\n",
            "[Epoch 7] Train Loss: 0.5759318087895712\n",
            "[Epoch 7] Val Loss: 0.7051472538312277\n",
            "[Epoch 8] Train Loss: 0.5507594144609239\n",
            "[Epoch 8] Val Loss: 0.7293712181250255\n",
            "[Epoch 9] Train Loss: 0.5260124085744222\n",
            "[Epoch 9] Val Loss: 0.7501742374897004\n",
            "[Epoch 10] Train Loss: 0.5027230452961392\n",
            "[Epoch 10] Val Loss: 0.7736876887480418\n",
            "0.6905115958849589\n",
            "Test Loss: 0.6847688097953797\n",
            "Test Accuracy: 0.6965\n"
          ]
        }
      ],
      "source": [
        "fnn = ForwardNeuralNetwork(input_size=300*10, dropout=0.3).to(device)\n",
        "train(fnn, train_loader, val_loader, test_loader, epoch_num=7, lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zt2yoBt5jz5"
      },
      "source": [
        "# RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "q8u9akT75lg5"
      },
      "outputs": [],
      "source": [
        "class myRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, n_layers=1, dropout=0.2):\n",
        "        super(myRNN, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "        self.layer1 = nn.RNN(input_size, hidden_size, n_layers, batch_first=True, nonlinearity='relu')\n",
        "        self.layer2 = nn.Linear(hidden_size, output_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        hidden = self.init_hidden(batch_size)\n",
        "        x = x.view(batch_size, -1, self.input_size)\n",
        "        x, hidden = self.layer1(x, hidden)\n",
        "        x = self.dropout(x)\n",
        "        x = self.layer2(x[:, -1, :])\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        return x\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return torch.zeros(self.n_layers, batch_size, self.hidden_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "cZg-I4-RZx02"
      },
      "outputs": [],
      "source": [
        "train_loader, val_loader, test_loader = gen_train_test_loader(test_df[[\"rnn_emb\",\"label\"]], batch_size=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4tTaxl452EJ",
        "outputId": "2d01d5f4-bfc3-4177-de3c-6e6a4ab07575"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 1] Train Loss: 1.0802663766013252\n",
            "[Epoch 1] Val Loss: 0.9088628629048665\n",
            "[Epoch 2] Train Loss: 0.8709351355234782\n",
            "[Epoch 2] Val Loss: 0.8131041674613952\n",
            "[Epoch 3] Train Loss: 0.7944715034696791\n",
            "[Epoch 3] Val Loss: 0.773356876373291\n",
            "[Epoch 4] Train Loss: 0.7610397662056817\n",
            "[Epoch 4] Val Loss: 0.7353272897402445\n",
            "[Epoch 5] Train Loss: 0.7351154816415575\n",
            "[Epoch 5] Val Loss: 0.733051476319631\n",
            "[Epoch 6] Train Loss: 0.7148104082213508\n",
            "[Epoch 6] Val Loss: 0.6991502415339153\n",
            "[Epoch 7] Train Loss: 0.6982838567097982\n",
            "[Epoch 7] Val Loss: 0.6818477902412414\n",
            "[Epoch 8] Train Loss: 0.6869051894611783\n",
            "[Epoch 8] Val Loss: 0.6661720527013143\n",
            "[Epoch 9] Train Loss: 0.6735935222307841\n",
            "[Epoch 9] Val Loss: 0.6682776913642884\n",
            "[Epoch 10] Train Loss: 0.6611515500280593\n",
            "[Epoch 10] Val Loss: 0.6587955342928569\n",
            "[Epoch 11] Train Loss: 0.6552951300409106\n",
            "[Epoch 11] Val Loss: 0.6586754361788432\n",
            "[Epoch 12] Train Loss: 0.6543201027446323\n",
            "[Epoch 12] Val Loss: 0.6671794934272766\n",
            "[Epoch 13] Train Loss: 0.6488319368892246\n",
            "[Epoch 13] Val Loss: 0.6676867097218832\n",
            "[Epoch 14] Train Loss: 0.6386240679952834\n",
            "[Epoch 14] Val Loss: 0.6464987270037333\n",
            "[Epoch 15] Train Loss: 0.6339892355600992\n",
            "[Epoch 15] Val Loss: 0.6419109779993694\n",
            "0.6419109779993694\n",
            "Test Loss: 0.6373978192011516\n",
            "Test Accuracy: 0.728\n"
          ]
        }
      ],
      "source": [
        "rnn = myRNN(300, 20, 3, n_layers=1, dropout=0.2)\n",
        "rnn.to(device)\n",
        "train(rnn, train_loader, val_loader, test_loader, epoch_num=15, lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkytNtrTnTa6"
      },
      "source": [
        "## GRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Az2QVoZnnUwz"
      },
      "outputs": [],
      "source": [
        "class myGRU(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, n_layers=1, dropout=0.2):\n",
        "        super(myGRU, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "        self.layer1 = nn.GRU(input_size, hidden_size, n_layers, batch_first=True)\n",
        "        self.layer2 = nn.Linear(hidden_size, output_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        hidden = self.init_hidden(batch_size)\n",
        "        x = x.view(batch_size, -1, self.input_size)\n",
        "        x, hidden = self.layer1(x, hidden)\n",
        "        x = self.dropout(x)\n",
        "        #x = x.contiguous().view(-1, self.hidden_dim)\n",
        "        x = self.layer2(x[:, -1, :])\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        return x\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return torch.zeros(self.n_layers, batch_size, self.hidden_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sy0OKY-9nmZd"
      },
      "outputs": [],
      "source": [
        "train_loader, val_loader, test_loader = gen_train_test_loader(sampled_df[[\"rnn_emb\",\"label\"]].sample(60000), batch_size=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-hyCfRxnmZe",
        "outputId": "2fdd8b71-be86-4a28-f156-37c04bd41af2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 1] Train Loss: 0.9135241704516941\n",
            "[Epoch 1] Val Loss: 0.7000537357330322\n",
            "[Epoch 2] Train Loss: 0.6907125168906317\n",
            "[Epoch 2] Val Loss: 0.6515696303049723\n",
            "[Epoch 3] Train Loss: 0.6555210551685757\n",
            "[Epoch 3] Val Loss: 0.6322451899846395\n",
            "[Epoch 4] Train Loss: 0.6360423141055637\n",
            "[Epoch 4] Val Loss: 0.6234472675323486\n",
            "[Epoch 5] Train Loss: 0.6196180650658077\n",
            "[Epoch 5] Val Loss: 0.6086734843254089\n",
            "[Epoch 6] Train Loss: 0.6099090187284681\n",
            "[Epoch 6] Val Loss: 0.6082295484542847\n",
            "[Epoch 7] Train Loss: 0.6014884813096788\n",
            "[Epoch 7] Val Loss: 0.599206537882487\n",
            "[Epoch 8] Train Loss: 0.594681343237559\n",
            "[Epoch 8] Val Loss: 0.5931967439651489\n",
            "[Epoch 9] Train Loss: 0.5885585050582886\n",
            "[Epoch 9] Val Loss: 0.5993342148462931\n",
            "[Epoch 10] Train Loss: 0.5820469205644395\n",
            "[Epoch 10] Val Loss: 0.590727757136027\n",
            "[Epoch 11] Train Loss: 0.5759189331266615\n",
            "[Epoch 11] Val Loss: 0.5888265194892883\n",
            "[Epoch 12] Train Loss: 0.5716812987327575\n",
            "[Epoch 12] Val Loss: 0.5919062320391337\n",
            "[Epoch 13] Train Loss: 0.5651350673569573\n",
            "[Epoch 13] Val Loss: 0.5841524977684021\n",
            "[Epoch 14] Train Loss: 0.5623340030246311\n",
            "[Epoch 14] Val Loss: 0.5932335805892944\n",
            "[Epoch 15] Train Loss: 0.561972583770752\n",
            "[Epoch 15] Val Loss: 0.5890633014043172\n",
            "0.5841524977684021\n",
            "Test Loss: 0.6000938518047333\n",
            "Test Accuracy: 0.74075\n"
          ]
        }
      ],
      "source": [
        "rnn = myGRU(300, 20, 3, n_layers=1, dropout=0.3)\n",
        "rnn.to(device)\n",
        "train(rnn, train_loader, val_loader, test_loader, epoch_num=15, lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2f2V2ITtYN2"
      },
      "source": [
        "## LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVHT0zsStX50"
      },
      "outputs": [],
      "source": [
        "class myLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, n_layers=1, dropout=0.2):\n",
        "        super(myLSTM, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "        self.layer1 = nn.LSTM(input_size, hidden_size, n_layers, batch_first=True)\n",
        "        self.layer2 = nn.Linear(hidden_size, output_size)\n",
        "        self.sig = nn.Sigmoid()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        hidden = self.init_hidden(batch_size)\n",
        "        x = x.view(batch_size, -1, self.input_size)\n",
        "        x, hidden = self.layer1(x, hidden)\n",
        "        x = self.dropout(x)\n",
        "        x = self.layer2(x[:, -1, :])\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        return x\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        h0 = torch.zeros((self.n_layers,batch_size,self.hidden_size)).to(device)\n",
        "        c0 = torch.zeros((self.n_layers,batch_size,self.hidden_size)).to(device)\n",
        "        hidden = (h0,c0)\n",
        "        return hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hm_kDDxZwJLN"
      },
      "outputs": [],
      "source": [
        "train_loader, val_loader, test_loader = gen_train_test_loader(sampled_df[[\"rnn_emb\",\"label\"]].sample(60000), batch_size=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxxAV5hswJLN",
        "outputId": "41068712-180b-46b1-e31f-519876c74b50"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 1] Train Loss: 0.8904835794766744\n",
            "[Epoch 1] Val Loss: 0.7527339668273926\n",
            "[Epoch 2] Train Loss: 0.7216494943300883\n",
            "[Epoch 2] Val Loss: 0.7012647609710694\n",
            "[Epoch 3] Train Loss: 0.6794473994572957\n",
            "[Epoch 3] Val Loss: 0.6808966921170553\n",
            "[Epoch 4] Train Loss: 0.6562810870276558\n",
            "[Epoch 4] Val Loss: 0.6736346255938213\n",
            "[Epoch 5] Train Loss: 0.6380674528545803\n",
            "[Epoch 5] Val Loss: 0.6466740821202596\n",
            "[Epoch 6] Train Loss: 0.6264161426756117\n",
            "[Epoch 6] Val Loss: 0.6452412053743998\n",
            "[Epoch 7] Train Loss: 0.6166972374386258\n",
            "[Epoch 7] Val Loss: 0.6355683681170146\n",
            "[Epoch 8] Train Loss: 0.6050843118561638\n",
            "[Epoch 8] Val Loss: 0.6251179626782735\n",
            "[Epoch 9] Train Loss: 0.6000131556193034\n",
            "[Epoch 9] Val Loss: 0.6296871143976848\n",
            "[Epoch 10] Train Loss: 0.5929494455125597\n",
            "[Epoch 10] Val Loss: 0.628282518227895\n",
            "[Epoch 11] Train Loss: 0.5872453300688002\n",
            "[Epoch 11] Val Loss: 0.6162628966967265\n",
            "[Epoch 12] Train Loss: 0.5822600102424622\n",
            "[Epoch 12] Val Loss: 0.620447011311849\n",
            "[Epoch 13] Train Loss: 0.5773513350221846\n",
            "[Epoch 13] Val Loss: 0.6123779908816019\n",
            "[Epoch 14] Train Loss: 0.5712117629051209\n",
            "[Epoch 14] Val Loss: 0.6084766181310018\n",
            "[Epoch 15] Train Loss: 0.5667681210835774\n",
            "[Epoch 15] Val Loss: 0.6166614745457967\n",
            "0.6084766181310018\n",
            "Test Loss: 0.6062604201634725\n",
            "Test Accuracy: 0.7375833333333334\n"
          ]
        }
      ],
      "source": [
        "rnn = myLSTM(300, 20, 3, n_layers=1, dropout=0.2)\n",
        "rnn.to(device)\n",
        "train(rnn, train_loader, val_loader, test_loader, epoch_num=15, lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YnUSDHVIwGA9"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "P3Jbpq4gpgHK",
        "ImIIwP4MU-_W",
        "pKUfdHFlItQb",
        "cm_j88RnIxhE",
        "pBlMVpPcHWi4",
        "5iJ72zukOgKv",
        "PCcptNuNpgHQ",
        "OftfBLx1L8eF",
        "RK6b3NqGUhh5",
        "vQb94SJxpgHQ",
        "FB_JYmGypgHR"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "NLP",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "ce56739b024e9597296b6666b65e03f262f75084e26dc2406f2b2f42b872b31f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
