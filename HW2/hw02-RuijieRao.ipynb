{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW02: HMM\n",
    "--by Ruijie Rao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_DIR+\"train\", sep=\"\\t\", names=[\"position_index\",\"word\",\"tag\"])\n",
    "dev_df = pd.read_csv(DATA_DIR+\"dev\", sep=\"\\t\", names=[\"position_index\",\"word\",\"tag\"])\n",
    "test_df = pd.read_csv(DATA_DIR+\"test\", sep=\"\\t\", names=[\"position_index\",\"word\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>position_index</th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Pierre</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Vinken</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>61</td>\n",
       "      <td>CD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>years</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   position_index    word  tag\n",
       "0               1  Pierre  NNP\n",
       "1               2  Vinken  NNP\n",
       "2               3       ,    ,\n",
       "3               4      61   CD\n",
       "4               5   years  NNS"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "def remove_punctuations(df):\n",
    "    punct_set = list(punctuation)+[\"''\",\"``\"]\n",
    "    df = df[~df[\"word\"].isin(punct_set)]\n",
    "    try:\n",
    "        df = df[~df[\"tag\"].isin(punct_set)]\n",
    "    except:\n",
    "        pass\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_df = train_df.groupby(\"word\").count()[[\"tag\"]].rename(columns={'tag':'occurences'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43193"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ruiji\\AppData\\Local\\Temp\\ipykernel_22208\\4132589229.py:3: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  filtered_vocab = filtered_vocab.append({'word':\"< unk >\",'occurences':vocab_df[vocab_df[\"occurences\"]<thres][\"occurences\"].sum()}, ignore_index=True)\\\n"
     ]
    }
   ],
   "source": [
    "thres = 2\n",
    "filtered_vocab = vocab_df[vocab_df[\"occurences\"]>=thres]\n",
    "filtered_vocab = filtered_vocab.append({'word':\"< unk >\",'occurences':vocab_df[vocab_df[\"occurences\"]<thres][\"occurences\"].sum()}, ignore_index=True)\\\n",
    "    .sort_values(by=\"occurences\", ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23183"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    20011\n",
       "Name: occurences, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_vocab.loc[filtered_vocab[\"word\"]==\"< unk >\",\"occurences\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_vocab_list = filtered_vocab[\"word\"].to_list()\n",
    "filtered_vocab_list += [\"< -unk >\", \"< allcap-unk >\", \"< Title-unk >\"]\n",
    "def pseudo_words_encoding(word, filtered_vocab_list):\n",
    "    if word in filtered_vocab_list:\n",
    "        return word\n",
    "    elif \"-\" in word:\n",
    "        return \"< -unk >\"\n",
    "    elif word.isupper():\n",
    "        return \"< allcap-unk >\"\n",
    "    elif word[0].isupper():\n",
    "        return \"< Title-unk >\"\n",
    "    else: return \"< unk >\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_filtered = train_df.copy()\n",
    "#train_df_filtered.loc[~train_df_filtered[\"word\"].isin(filtered_vocab_list), \"word\"] = \"< unk >\"\n",
    "train_df_filtered[\"word\"] = train_df[\"word\"].apply(lambda x: pseudo_words_encoding(x,filtered_vocab_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_vocab.to_csv(DATA_DIR+\"vocab.txt\", index=True, sep=\"\\t\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the selected threshold for unknown words replacement?\n",
    "- 5\n",
    "\n",
    "What is the total size of your vocabulary and what is the total occurrences of the special token ‘< unk >’ after replacement?\n",
    "- 11688 and 50296\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HMM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of States\n",
    "from collections import Counter\n",
    "count_states = Counter(train_df_filtered[\"tag\"].values)\n",
    "count_states[\"< start >\"] = train_df_filtered[train_df_filtered[\"position_index\"]==1][\"word\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of s -> s'\n",
    "transition_vocab = train_df_filtered.rename(columns={'tag':'state'}).copy()\n",
    "next_state = transition_vocab[\"state\"].copy()\n",
    "transition_vocab[\"state'\"] = next_state[1:].reset_index(drop=True)\n",
    "initial_selection = (transition_vocab[\"position_index\"]==1)[1:].reset_index(drop=True)\n",
    "initial_selection.loc[len(initial_selection)] = True\n",
    "transition_vocab.loc[initial_selection, \"state\"] = \"< start >\"\n",
    "transition_vocab.loc[len(transition_vocab)-1, \"state'\"] = transition_vocab.loc[0, \"state\"]\n",
    "transition_count = transition_vocab.groupby([\"state\",\"state'\"])[[\"word\"]].count().reset_index()\\\n",
    "    .rename(columns={\"word\":\"occurences\"}).sort_values(by=\"occurences\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transition Prob\n",
    "def t_prob(x):\n",
    "        return x.occurences/count_states[x.state]\n",
    "transition_prob = transition_count\n",
    "transition_prob[\"p\"] = transition_prob.apply(t_prob, axis=1)\n",
    "#transition_prob[\"key\"] = transition_prob.apply(lambda x: str((x.state,x[\"state'\"])), axis=1)\n",
    "transition_prob[\"key\"] = transition_prob.apply(lambda x: (x.state,x[\"state'\"]), axis=1)\n",
    "transition_prob.set_index(\"key\", inplace=True)\n",
    "transition_dict = transition_prob[[\"p\"]].to_dict()[\"p\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of s -> x\n",
    "emission_vocab = train_df_filtered.rename(columns={'tag':'state'}).copy()\n",
    "emission_count = emission_vocab.groupby([\"state\",\"word\"])[[\"position_index\"]].count().reset_index()\\\n",
    "    .rename(columns={\"position_index\":\"occurences\"}).sort_values(by=\"occurences\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emission Prob\n",
    "emission_prob = emission_count\n",
    "#emission_prob[\"key\"] = emission_prob.apply(lambda x: str((x.state,x.word)), axis=1)\n",
    "emission_prob[\"key\"] = emission_prob.apply(lambda x: (x.state,x.word), axis=1)\n",
    "emission_prob.set_index(\"key\", inplace=True)\n",
    "emission_prob[\"p\"] = emission_prob.apply(lambda x: x.occurences/count_states[x.state], axis=1)\n",
    "emission_dict = emission_prob[[\"p\"]].to_dict()[\"p\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hmm_model_output = {\"transition\": transition_dict, \"emission\": emission_dict}\\nwith open(DATA_DIR+\"hmm.json\", \"w\") as file:\\n    json.dump(hmm_model_output, file)'"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output\n",
    "'''hmm_model_output = {\"transition\": transition_dict, \"emission\": emission_dict}\n",
    "with open(DATA_DIR+\"hmm.json\", \"w\") as file:\n",
    "    json.dump(hmm_model_output, file)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1392"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "30362"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# How many transition and emission parameters in your HMM?\n",
    "display(len(transition_dict))\n",
    "display(len(emission_dict))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_list = [t for t in count_states.keys() if t != \"< start >\"]\n",
    "K  = len(tag_list)\n",
    "V = len(filtered_vocab_list)\n",
    "transition_matrix = np.zeros((K,K))\n",
    "emission_matrix = np.zeros((K,V))\n",
    "initial_array = np.zeros((1,K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transition_matrix & initial_array\n",
    "for key, value in transition_dict.items():\n",
    "    state, state_ = key\n",
    "    j = tag_list.index(state_)\n",
    "    if state == \"< start >\":\n",
    "        initial_array[0,j] = value\n",
    "        continue\n",
    "    i = tag_list.index(state)\n",
    "    transition_matrix[i,j] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emission_matrix\n",
    "for key, value in emission_dict.items():\n",
    "    state, word = key\n",
    "    j = filtered_vocab_list.index(word)\n",
    "    i = tag_list.index(state)\n",
    "    emission_matrix[i,j] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decoding(test_df, initial_array, transition_matrix, emission_matrix, filtered_vocab_list):\n",
    "    result = []\n",
    "    for index, row in test_df.iterrows():\n",
    "        if row.position_index == 1:\n",
    "            prev_state_index = 0\n",
    "            t_vec = initial_array\n",
    "        else:\n",
    "            t_vec = transition_matrix[prev_state_index,:]\n",
    "        word_index = filtered_vocab_list.index(pseudo_words_encoding(row.word, filtered_vocab_list))\n",
    "        e_vec = emission_matrix[:,word_index]\n",
    "        argmax_index = np.argmax(t_vec*e_vec.T)\n",
    "        prev_state_index = argmax_index\n",
    "        argmax_tag = tag_list[argmax_index]\n",
    "        result.append(argmax_tag)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_pred = greedy_decoding(dev_df, initial_array, transition_matrix, emission_matrix, filtered_vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(prediction, ground_truth):\n",
    "    return sum(np.array(prediction) == np.array(ground_truth))/len(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9412300406775544"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(dev_pred, dev_df[\"tag\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df[\"greedy\"] = dev_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output\n",
    "test_df[\"greedy\"] = greedy_decoding(test_df, initial_array, transition_matrix, emission_matrix, filtered_vocab_list)\n",
    "test_df[[\"position_index\",\"word\",\"greedy\"]].to_csv(\"greedy.out\", header=False, index=False, sep=\"\\t\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viterbi Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corpus(words):\n",
    "    corpus = []\n",
    "    line = []\n",
    "    for word in words:\n",
    "        line.append(word)\n",
    "        if word==\".\":\n",
    "            corpus.append(line)\n",
    "            line = []\n",
    "    return corpus\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Viterbi:\n",
    "    def __init__(self, t_dict, e_dict, vocab, tags) -> None:\n",
    "        self.t_dict= t_dict\n",
    "        self.e_dict = e_dict\n",
    "        self.vocab = vocab\n",
    "        self.tags = tags # without < start >\n",
    "        self.K = len(self.tags)\n",
    "\n",
    "    def predict(self, corpus:list[str]):\n",
    "        N = len(corpus)\n",
    "        M = np.zeros((N, self.K)) # Num of words x Num of tags\n",
    "        states_record = np.zeros((N-1, self.K)) # Records the best previous states except for the initial\n",
    "        for i, word_ in enumerate(corpus):\n",
    "            word = pseudo_words_encoding(word_, self.vocab)\n",
    "            for j in range(self.K):\n",
    "                tag = self.tags[j]\n",
    "                if i == 0: # initial state\n",
    "                    M[i,j] = self.t_dict.get(('< start >', tag),0) * self.e_dict.get((tag, word),0)\n",
    "                else:\n",
    "                    temp = [M[i-1,k]*self.t_dict.get((tag_,tag),0)*self.e_dict.get((tag, word),0) for k,tag_ in enumerate(self.tags)]\n",
    "                    best_prev_state_i = np.argmax(temp)\n",
    "                    states_record[i-1,j] = best_prev_state_i\n",
    "                    M[i,j] = temp[best_prev_state_i]\n",
    "        # Backward\n",
    "        result = []\n",
    "        best_state_i = np.argmax(M[-1,:])\n",
    "        result.append(self.tags[best_state_i])\n",
    "        for i in range(len(states_record)-1,-1,-1):\n",
    "            best_state_i = int(states_record[i,best_state_i])\n",
    "            result.append(self.tags[best_state_i])\n",
    "        return result[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "myViterbi = Viterbi(transition_dict, emission_dict, filtered_vocab_list, tag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9498664319106308"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_pred = []\n",
    "for sentence in create_corpus(dev_df[\"word\"].values):\n",
    "    dev_pred += myViterbi.predict(sentence)\n",
    "accuracy(dev_pred, dev_df[\"tag\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for sentence in create_corpus(test_df[\"word\"].values):\n",
    "    result += myViterbi.predict(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output\n",
    "test_df[\"viterbi\"] = result\n",
    "test_df[[\"position_index\",\"word\",\"viterbi\"]].to_csv(\"viterbi.out\", header=False, index=False, sep=\"\\t\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df[\"viterbi\"] = dev_pred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df[\"filtered_word\"] = dev_df[\"word\"]\n",
    "dev_df.loc[~dev_df[\"word\"].isin(filtered_vocab_list), \"filtered_word\"] = \"< unk >\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_set = dev_df[dev_df[\"tag\"]!=dev_df[\"greedy\"]][[\"position_index\",\"filtered_word\",\"word\",\"tag\",\"greedy\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tag   greedy\n",
       "NN    JJ        531\n",
       "JJ    NN        394\n",
       "NN    NNP       346\n",
       "VBN   VBD       313\n",
       "VBD   VBN       270\n",
       "NNS   NN        263\n",
       "WDT   IN        248\n",
       "RB    IN        220\n",
       "NNP   JJ        213\n",
       "JJ    NNP       200\n",
       "NN    CD        184\n",
       "IN    RB        151\n",
       "JJ    VBN       145\n",
       "      CD        141\n",
       "NNS   CD        139\n",
       "      NNP       139\n",
       "VBP   VB        138\n",
       "RB    JJ        134\n",
       "NNPS  NNP       127\n",
       "VBG   NN        127\n",
       "Name: word, dtype: int64"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_set.groupby([\"tag\",\"greedy\"]).count()[\"word\"].sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>position_index</th>\n",
       "      <th>filtered_word</th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "      <th>greedy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>771</th>\n",
       "      <td>29</td>\n",
       "      <td>editorial</td>\n",
       "      <td>editorial</td>\n",
       "      <td>JJ</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813</th>\n",
       "      <td>7</td>\n",
       "      <td>&lt; unk &gt;</td>\n",
       "      <td>irreverent</td>\n",
       "      <td>JJ</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>18</td>\n",
       "      <td>close</td>\n",
       "      <td>close</td>\n",
       "      <td>JJ</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828</th>\n",
       "      <td>22</td>\n",
       "      <td>editorial</td>\n",
       "      <td>editorial</td>\n",
       "      <td>JJ</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>17</td>\n",
       "      <td>&lt; unk &gt;</td>\n",
       "      <td>bona</td>\n",
       "      <td>JJ</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>18</td>\n",
       "      <td>&lt; unk &gt;</td>\n",
       "      <td>fide</td>\n",
       "      <td>JJ</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1109</th>\n",
       "      <td>10</td>\n",
       "      <td>editorial</td>\n",
       "      <td>editorial</td>\n",
       "      <td>JJ</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1327</th>\n",
       "      <td>6</td>\n",
       "      <td>&lt; unk &gt;</td>\n",
       "      <td>shoddy</td>\n",
       "      <td>JJ</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1328</th>\n",
       "      <td>7</td>\n",
       "      <td>editorial</td>\n",
       "      <td>editorial</td>\n",
       "      <td>JJ</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1720</th>\n",
       "      <td>22</td>\n",
       "      <td>bulk</td>\n",
       "      <td>bulk</td>\n",
       "      <td>JJ</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970</th>\n",
       "      <td>20</td>\n",
       "      <td>female</td>\n",
       "      <td>female</td>\n",
       "      <td>JJ</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2132</th>\n",
       "      <td>10</td>\n",
       "      <td>right</td>\n",
       "      <td>right</td>\n",
       "      <td>JJ</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>5</td>\n",
       "      <td>core</td>\n",
       "      <td>core</td>\n",
       "      <td>JJ</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3408</th>\n",
       "      <td>56</td>\n",
       "      <td>flagship</td>\n",
       "      <td>flagship</td>\n",
       "      <td>JJ</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3477</th>\n",
       "      <td>27</td>\n",
       "      <td>&lt; unk &gt;</td>\n",
       "      <td>lone</td>\n",
       "      <td>JJ</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4173</th>\n",
       "      <td>7</td>\n",
       "      <td>backup</td>\n",
       "      <td>backup</td>\n",
       "      <td>JJ</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4483</th>\n",
       "      <td>23</td>\n",
       "      <td>downtown</td>\n",
       "      <td>downtown</td>\n",
       "      <td>JJ</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4814</th>\n",
       "      <td>32</td>\n",
       "      <td>&lt; unk &gt;</td>\n",
       "      <td>appreciable</td>\n",
       "      <td>JJ</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6643</th>\n",
       "      <td>10</td>\n",
       "      <td>desktop</td>\n",
       "      <td>desktop</td>\n",
       "      <td>JJ</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6744</th>\n",
       "      <td>28</td>\n",
       "      <td>downtown</td>\n",
       "      <td>downtown</td>\n",
       "      <td>JJ</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6745</th>\n",
       "      <td>29</td>\n",
       "      <td>&lt; unk &gt;</td>\n",
       "      <td>pyramid-shaped</td>\n",
       "      <td>JJ</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6928</th>\n",
       "      <td>57</td>\n",
       "      <td>&lt; unk &gt;</td>\n",
       "      <td>inoperable</td>\n",
       "      <td>JJ</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6948</th>\n",
       "      <td>10</td>\n",
       "      <td>video</td>\n",
       "      <td>video</td>\n",
       "      <td>JJ</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7044</th>\n",
       "      <td>20</td>\n",
       "      <td>&lt; unk &gt;</td>\n",
       "      <td>gruesome</td>\n",
       "      <td>JJ</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7422</th>\n",
       "      <td>28</td>\n",
       "      <td>&lt; unk &gt;</td>\n",
       "      <td>calamitous</td>\n",
       "      <td>JJ</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8210</th>\n",
       "      <td>10</td>\n",
       "      <td>aerospace</td>\n",
       "      <td>aerospace</td>\n",
       "      <td>JJ</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8307</th>\n",
       "      <td>6</td>\n",
       "      <td>&lt; unk &gt;</td>\n",
       "      <td>15th</td>\n",
       "      <td>JJ</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8363</th>\n",
       "      <td>23</td>\n",
       "      <td>textile</td>\n",
       "      <td>textile</td>\n",
       "      <td>JJ</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8561</th>\n",
       "      <td>40</td>\n",
       "      <td>textile</td>\n",
       "      <td>textile</td>\n",
       "      <td>JJ</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8575</th>\n",
       "      <td>12</td>\n",
       "      <td>textile</td>\n",
       "      <td>textile</td>\n",
       "      <td>JJ</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      position_index filtered_word            word tag greedy\n",
       "771               29     editorial       editorial  JJ     NN\n",
       "813                7       < unk >      irreverent  JJ     NN\n",
       "824               18         close           close  JJ     NN\n",
       "828               22     editorial       editorial  JJ     NN\n",
       "875               17       < unk >            bona  JJ     NN\n",
       "876               18       < unk >            fide  JJ     NN\n",
       "1109              10     editorial       editorial  JJ     NN\n",
       "1327               6       < unk >          shoddy  JJ     NN\n",
       "1328               7     editorial       editorial  JJ     NN\n",
       "1720              22          bulk            bulk  JJ     NN\n",
       "1970              20        female          female  JJ     NN\n",
       "2132              10         right           right  JJ     NN\n",
       "2442               5          core            core  JJ     NN\n",
       "3408              56      flagship        flagship  JJ     NN\n",
       "3477              27       < unk >            lone  JJ     NN\n",
       "4173               7        backup          backup  JJ     NN\n",
       "4483              23      downtown        downtown  JJ     NN\n",
       "4814              32       < unk >     appreciable  JJ     NN\n",
       "6643              10       desktop         desktop  JJ     NN\n",
       "6744              28      downtown        downtown  JJ     NN\n",
       "6745              29       < unk >  pyramid-shaped  JJ     NN\n",
       "6928              57       < unk >      inoperable  JJ     NN\n",
       "6948              10         video           video  JJ     NN\n",
       "7044              20       < unk >        gruesome  JJ     NN\n",
       "7422              28       < unk >      calamitous  JJ     NN\n",
       "8210              10     aerospace       aerospace  JJ     NN\n",
       "8307               6       < unk >            15th  JJ     NN\n",
       "8363              23       textile         textile  JJ     NN\n",
       "8561              40       textile         textile  JJ     NN\n",
       "8575              12       textile         textile  JJ     NN"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_set[(error_set[\"tag\"]==\"JJ\") & (error_set[\"greedy\"]==\"NN\")].head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005253501027177067"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emission_dict[(\"NN\",\"< -unk >\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_set = dev_df[dev_df[\"filtered_word\"]=='< unk >']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6050286441756842"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(unk_set[\"greedy\"],unk_set[\"tag\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19789104610393007"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transition_dict[(\"< start >\",\"NNP\")]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0 (default, Nov 15 2020, 08:30:55) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ce56739b024e9597296b6666b65e03f262f75084e26dc2406f2b2f42b872b31f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
